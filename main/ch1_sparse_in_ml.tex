\chapter{Sparse Data and Formats}
\section{Definition}

Data is said sparse when it is contains only a few non-zero values. That kind of dataset is really common in Machine Learning applications and can be an high influence on the computation. Common use-cases include recommendation systems, clickstream dataset,\dots

The sparsity of a dataset is defined by :

\begin{equation}\label{eqn:sparsity}
sparsity = \frac{\text{\# non-zero values}}{\text{\# values}}
\end{equation}

Conversely when a dataset has only a few null values, the data are said dense. The density of the dataset is defined by the inverse of the sparsity:
\begin{equation}\label{eqn:density}
density = \frac{1}{\text{sparsity}}
\end{equation}

Using dense methods and data structure with sparse data dismisses all optimization and performance gains we could achieve using sparse linear algebra.

\section{The Advantages of Sparse Data}
Linear algebra is a branch of mathematics that is widely used throughout science and engineering. MacÄ¥ine learning and more specifically deep learning algorithms rely on linear algebra. Sparsity is a very useful property in Machine Learning. Some algorithms can have fast optimization, fast evaluation of the model, statistical robustness or other computational advantages.

A lot of machine learning applications are using sparse dataset such as recommender system. Natural language processing algorithm tends to have sparse dataset as stated by the Zipf's law \cite{Zipf}.

\section{Sparse Data are very common in Machine Learning}

In Machine Learning it's very common to deal with sparse dataset. We can encounter them in any kind of applications: Natural Language Processing, Retrieving Systems, Recommender Systems, etc.

Given the possible optimization that sparse dataset allows and the high number of people that could take advantages of it, it becomes important to add the support of sparse data in the DL4J suite of libraries. 
\subsection{A Real Case of Sparse Dataset}
In 2008 Netflix launched a contest, the Netflix Grand Prize \cite{netflixgrandprize}, to improve their recommender system model and to increase the accuracy of predictions. They published an sample dataset made with the ratings of anonymous Netflix customers. The dataset had more than 100 millions sampled ratings and it contained about $m=480'186$ users and $m=17'770$ movies \cite{Koren091the}. If stored as a dense matrix, it would need to store $8'532'905'220$ values in memory. That corresponds to a sparsity $\cong \frac{100'000'000}{8'532'905'220} = 0.011719338$.

Storing more than 8 trillions 64-bit floating-point numbers needs more than 64 gigabyte of memory which quickly become unmanageable even for the world's fastest supercomputers. 

\section{Solution: Sparse Representation and Operations}
Even if the compression of sparse data is very efficient when storing as dense data, we could gain in performance during the processing by using a sparse representation with sparse operations. 

In sparse operations only the values are accessed and processed while in dense operations, every cell is accessed even if it contains a zero value.

To reduce the size of the memory needed and to gain in performance, we must store the data into a sparse format. There are different kind of formats which each of them is more suitable to different aspects (Storage vs computation).



\section{Formats}

There are many methods for storing sparse data, each of them presents different advantages and disadvantages.

\subsection{Matrices}
\subsubsection{Coordinates Format}

It is the simplest method to encode a sparse array. The coordinates and the value of each non-zero entry are stored in arrays.
Typically each element is encoded in a tuple (row, column, value)


\begin{figure}[h]
	\[
	A_{(M\times N)} = 
	\begin{bmatrix}
	0 &  2 & 0 \\
	0 &  0 & 3 \\
	1 &  0 & 4\\
	0 &  0 & 0
	\end{bmatrix}
	\quad\rightarrow\quad
	\begin{aligned}
	Values_{(1\times NNZ)} = 
	\begin{bmatrix}
	2 &  3 & 1 & 4
	\end{bmatrix}
	\\
	Rows_{(1\times NNZ)} = 
	\begin{bmatrix}
	0 &  1 & 2 & 2
	\end{bmatrix}
	\\
	Columns_{(1\times NNZ)} = 
	\begin{bmatrix}
	1 &  2 & 0 & 2
	\end{bmatrix}
	\end{aligned}
	\]
	\caption{A matrix stored in COO format}
	\label{fig:cooformat}
\end{figure}

This format provides an easy and fast way to retrieve a value and to insert a new non-zero element. It's also fast and simple to convert into a dense format.

But this format is not the most efficient regarding the memory consumption.

\subsubsection{Compressed Row Format}

The Compressed Row and the Compressed Column formats are the most general format to store a sparse array. They don't store any unnecessary element conversely to the COO format. But it requires more steps to access a element than the COO format. 

Similarly to the row-major ordering explained in section \ref{sec:storing}, this format stores the values by row and use pointers to differentiate each row.

Each non-zero element of a row are stored contiguously in the memory. Each row are also contiguously stored.

The format, described by the Intel MKL Sparse Library \cite{mklformat}, requires four arrays:
\begin{description}[leftmargin=!,labelwidth=\widthof{\bfseries Beginning of row pointers}]
	\item [Values] All the nonzero values are store contiguously in an array. The array size is {NNZ}.
	\item [Column pointers] This array keeps the column position for each values.
	\item [Beginning of row pointers] Each pointer $i$ points to the first element of the row $i$ in the values array. The array size is the number of rows of the array.
	\item [End of row pointers]  Each pointer $i$ points to the first element in the values array that does not belong to the row $i$ . The array size is the number of rows of the array.
\end{description}

\begin{figure}
	\[
	A_{(N\times M)} = 
	\begin{bmatrix}
	0 & 2 & 0 & 0\\
	0 & 0 & 3 & 0\\
	0 & 0 & 0 & 0\\
	1 & 0 & 4 & 0\\
	0 & 0 & 2 & 1
	\end{bmatrix}
	\quad\rightarrow\quad
	\begin{aligned}
	Values_{(1\times NNZ)} = 
	\begin{bmatrix}
	2 &  3 & 1 & 4 & 2 & 1
	\end{bmatrix}
	\\
	Columns_{(1\times NNZ)} = 
	\begin{bmatrix}
	1 &  2 & 0 & 2 & 2 & 3
	\end{bmatrix}
	\\
	pointersB_{(1\times N)} = 
	\begin{bmatrix}
	0 & 1 & 2 & 2 & 4 
	\end{bmatrix}
	\\
	PointersE_{(1\times N)} = 
	\begin{bmatrix}
	1 & 2 & 2 & 4 & 6
	\end{bmatrix}
	\\
	\end{aligned}
	\]
	\caption{A matrix stored in CSR format}
		\label{fig:csrformat}
\end{figure}
\subsubsection{Compressed Column Format}
The Compressed Column Format is similar to {CSR} but it compresses columns instead of rows. The values are stored in a column-major ordering, as explained in section \ref{sec:storing}.

Given a matrix $N\times M$, the pointers arrays will have a size $M$. 
\subsection{Tensors - Multi-dimensional arrays}

A tensor is a multi-dimensional array. The order of the tensor is its number of dimension. It corresponds to the number of indexes needed to index a value. Matrices, vectors and scalars can be represented as tensors where the order is equals to 2, 1 and 0 respectively.

This generalization allows a more generic implementation of a n-dimensional array in the Nd4j library.
\subsubsection{Coordinates Format} \label{sssec:coo}
The COO format can easily be extended to encode tensors by storing an array of indexes instead the row and column coordinates. 

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=2.5in]{images/tensorscooexpl.pdf} 
		\caption{A $3\times 3\times 3$ tensor}
        \label{fig:tensorCOO}
	\end{center}
\end{figure}
The tensor, illustrated in figure \ref{fig:tensorCOO}, of order $K = 3$ with shape $3\times 3 \times 3$ which has the following non-zero values :
\begin{center}
	\begin{tabular}{ l | c  }
		value & indexes\\ \hline
		1 & 0 1 0\\ \hline
		2 & 1 1 2 \\ \hline
		3 & 1 2 0 \\ \hline
		4 & 2 0 1 \\ \hline
		5 & 2 2 0 \\ 
		
	\end{tabular}
\end{center}

It can be encoded with one values array and one indexes array :
\begin{figure}[h]
	\[
	\begin{aligned}
	Values_{(1\times NNZ)} = 
	\begin{bmatrix}
	1, &  2, & 3, & 4, & 5
	\end{bmatrix}
	\\
	Indexes_{(NNZ \times K)} = 
	\begin{bmatrix}
	[0, 1, 0] ,&  [1, 1, 2], & [1, 2, 0], & [2, 0, 1], & [2, 2, 0]
	\end{bmatrix}
	\end{aligned}
	\]
	\caption{A tensor stored in COO format}
		\label{fig:cooformattensor}
\end{figure}

