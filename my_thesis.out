\BOOKMARK [0][-]{chapter*.2}{Abstract}{}% 1
\BOOKMARK [0][-]{section*.4}{List of figures}{}% 2
\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 3
\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 4
\BOOKMARK [1][-]{section.1.1}{Sparse Data is very common in Machine Learning}{chapter.1}% 5
\BOOKMARK [1][-]{section.1.2}{The Advantages of Sparse Data}{chapter.1}% 6
\BOOKMARK [2][-]{subsection.1.2.1}{A Real Case of Sparse Dataset}{section.1.2}% 7
\BOOKMARK [1][-]{section.1.3}{Solution: Sparse Representation and Operations}{chapter.1}% 8
\BOOKMARK [0][-]{chapter.2}{Sparsity and Formats}{}% 9
\BOOKMARK [1][-]{section.2.1}{Definition}{chapter.2}% 10
\BOOKMARK [1][-]{section.2.2}{Formats}{chapter.2}% 11
\BOOKMARK [2][-]{subsection.2.2.1}{Matrices}{section.2.2}% 12
\BOOKMARK [2][-]{subsection.2.2.2}{Tensors - Multi-dimensional arrays}{section.2.2}% 13
\BOOKMARK [0][-]{chapter.3}{The Deeplearning4j Library}{}% 14
\BOOKMARK [1][-]{section.3.1}{Architecture of the library}{chapter.3}% 15
\BOOKMARK [1][-]{section.3.2}{The Importance of Nd4j in the Library}{chapter.3}% 16
\BOOKMARK [1][-]{section.3.3}{Nd4j needs a Sparse Representation}{chapter.3}% 17
\BOOKMARK [0][-]{chapter.4}{Structure of an Multi-dimensional Array}{}% 18
\BOOKMARK [1][-]{section.4.1}{Storing an Array}{chapter.4}% 19
\BOOKMARK [2][-]{subsection.4.1.1}{Data Buffer}{section.4.1}% 20
\BOOKMARK [2][-]{subsection.4.1.2}{Parameters of an Array}{section.4.1}% 21
\BOOKMARK [1][-]{section.4.2}{Views}{chapter.4}% 22
\BOOKMARK [1][-]{section.4.3}{Indexes}{chapter.4}% 23
\BOOKMARK [1][-]{section.4.4}{Operations}{chapter.4}% 24
\BOOKMARK [0][-]{chapter.5}{Implementation}{}% 25
\BOOKMARK [1][-]{section.5.1}{Hierarchy of Arrays}{chapter.5}% 26
\BOOKMARK [1][-]{section.5.2}{Limitations and Constraints}{chapter.5}% 27
\BOOKMARK [2][-]{subsection.5.2.1}{Storing off-heap}{section.5.2}% 28
\BOOKMARK [2][-]{subsection.5.2.2}{Workspaces}{section.5.2}% 29
\BOOKMARK [2][-]{subsection.5.2.3}{DataBuffers have a fixed length}{section.5.2}% 30
\BOOKMARK [1][-]{section.5.3}{CSR Matrices Implementation}{chapter.5}% 31
\BOOKMARK [2][-]{subsection.5.3.1}{Structure}{section.5.3}% 32
\BOOKMARK [2][-]{subsection.5.3.2}{Put a value}{section.5.3}% 33
\BOOKMARK [2][-]{subsection.5.3.3}{Get a Sub-array}{section.5.3}% 34
\BOOKMARK [2][-]{subsection.5.3.4}{Limits of this format}{section.5.3}% 35
\BOOKMARK [1][-]{section.5.4}{COO Tensors}{chapter.5}% 36
\BOOKMARK [2][-]{subsection.5.4.1}{Naive implementation}{section.5.4}% 37
\BOOKMARK [2][-]{subsection.5.4.2}{Reverse coordinates}{section.5.4}% 38
\BOOKMARK [2][-]{subsection.5.4.3}{Put a Value}{section.5.4}% 39
\BOOKMARK [2][-]{subsection.5.4.4}{More parameters are needed to define the tensors}{section.5.4}% 40
\BOOKMARK [2][-]{subsection.5.4.5}{Sparse Indexes Translation}{section.5.4}% 41
\BOOKMARK [2][-]{subsection.5.4.6}{Computations of the the Parameters}{section.5.4}% 42
\BOOKMARK [2][-]{subsection.5.4.7}{Final Implementation}{section.5.4}% 43
\BOOKMARK [2][-]{subsection.5.4.8}{Get a Sub-Array}{section.5.4}% 44
\BOOKMARK [0][-]{chapter.6}{Operations}{}% 45
\BOOKMARK [1][-]{section.6.1}{Basic Linear Algerba Subprograms \(BLAS\)}{chapter.6}% 46
\BOOKMARK [1][-]{section.6.2}{Backends}{chapter.6}% 47
\BOOKMARK [2][-]{subsection.6.2.1}{In-Place Routines}{section.6.2}% 48
\BOOKMARK [2][-]{subsection.6.2.2}{Level 1 Routines}{section.6.2}% 49
\BOOKMARK [2][-]{subsection.6.2.3}{Level 2 and Level 3 Routines}{section.6.2}% 50
\BOOKMARK [1][-]{section.6.3}{Libnd4j}{chapter.6}% 51
\BOOKMARK [0][-]{chapter.7}{Results}{}% 52
\BOOKMARK [1][-]{section.7.1}{Storing a huge data set}{chapter.7}% 53
\BOOKMARK [1][-]{section.7.2}{Operations performance}{chapter.7}% 54
\BOOKMARK [2][-]{subsection.7.2.1}{level 1}{section.7.2}% 55
\BOOKMARK [2][-]{subsection.7.2.2}{level 2}{section.7.2}% 56
\BOOKMARK [0][-]{chapter.8}{Future}{}% 57
\BOOKMARK [1][-]{section.8.1}{Operations}{chapter.8}% 58
\BOOKMARK [1][-]{section.8.2}{Support of the GPU backend}{chapter.8}% 59
\BOOKMARK [1][-]{section.8.3}{Make the sparse array API compliant}{chapter.8}% 60
\BOOKMARK [1][-]{section.8.4}{Support More Sparse Formats}{chapter.8}% 61
\BOOKMARK [1][-]{section.8.5}{Tensor contraction indexing}{chapter.8}% 62
\BOOKMARK [1][-]{section.8.6}{Optimization}{chapter.8}% 63
\BOOKMARK [0][-]{chapter.9}{Conclusion}{}% 64
\BOOKMARK [0][-]{appendix.A}{Appendices}{}% 65
\BOOKMARK [1][-]{section.A.1}{Algorithm Execution Example}{appendix.A}% 66
\BOOKMARK [2][-]{subsection.A.1.1}{Sparse Offset Computation Algorithm}{section.A.1}% 67
\BOOKMARK [2][-]{subsection.A.1.2}{Indexes Translation Algorithm}{section.A.1}% 68
\BOOKMARK [1][-]{section.A.2}{Code Snippets}{appendix.A}% 69
\BOOKMARK [0][-]{appendix*.23}{Bibliography}{}% 70
